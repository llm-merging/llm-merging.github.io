---
layout: default
---

<p style='text-align: justify;'>

The competition will provide the participants with a list of **expert** models that have already been trained on a task-specific dataset. All of these models will be publicly available on the Hugging Face Model Hub with licenses that permit their use for research purposes. These models can either be fully fine-tuned models or models obtained by parameter-efficient fine-tuning methods such as LoRA. Models on this list will be required to satisfy the following criteria: (1) model size $\leq 8$B parameters, and (2) model with licenses compatible with research use (e.b., [MIT](https://spdx.org/licenses/MIT.html), [Apache 2](https://www.apache.org/licenses/LICENSE-2.0) etc). The goal of this competition is to re-use the provided models to create a generalist model that can perform well on a wide variety of skills like reasoning, coding, maths, chat, and tool use.
This list of models will include popular pre-trained models such as LLaMA-7B, Mistral-7B, and Gemma-7B. 


</p>

<p style='text-align: justify;'>
Along with these expert models, we also plan to provide two different types of datasets: (1) a list of **re-calibration datasets** to either tune the hyperparameters of merging methods, to perform additional training steps, to learn a routing, or to calibrate the final model, (2) a set of **validation tasks** that can be used to evaluate the final method's performance. The datasets will be released as part of the starter kit for the participants and are already hosted on the Hugging Face Hub with a permissive license. Apart from these, we will have two sets of hidden tasks that will be used to evaluate the submissions from participants: (1) a set of **leaderboard ranking test tasks**, and (2) a set of **final ranking test tasks**. The leaderboard ranking tasks will have some overlap with the test set tasks to provide an additional signal to the participants. 

<br>

The re-calibration datasets chosen are commonly used instruction-tuning datasets.
The validation datasets chosen are commonly used benchmarks for measuring math, code, question-answering, and reasoning. 

We will not collect or release any new datasets for training or evaluation as part of this competition.

</p>


## Validation Datasets Lists:

<!-- * GSM8K
* HumanEval
* TriviaQA -->
* BoolQ
* MAWPS

The main purpose of validation datasets is to measure the time and space efficiency. We are using a hidden list of test datasets to measure the performances.


<!-- * [Databricks-Dolly-15](https://huggingface.co/datasets/databricks/databricks-dolly-15k)
* [OpenAssistant Conversations Dataset (oasst1)](https://huggingface.co/datasets/OpenAssistant/oasst1)
* [The Flan Collection](https://github.com/google-research/FLAN/tree/main/flan/v2)
* [AllenAI Dolma](https://huggingface.co/datasets/allenai/dolma)
* [RedPajama-Data-1T](https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T)
* [LIMA](https://huggingface.co/datasets/GAIR/lima) -->

<br>

<!-- ## Tasks and application scenarios

More details will be released soon! -->

<!-- <p style='text-align: justify;'>
Under no circumstances should you use data that infringes upon data usage agreements, copyright laws, or privacy policies. This means you should not use datasets that utilize generated content, whether in the form of instructions/prompts or results/answers from another LLM if that LLM did not have a permissive license that explicitly allowed you to do so. If you opt to create your own dataset, it must be open-sourced and readily accessible to the general public at the time of submission. Some concrete clarifications: 
</p>


* Any generated llm dataset must be generated from one of the approved base models
* You can under no circumstance use datasets generated by ChatGPT
* You can generate a dataset with Llama2 if you make sure that your dataset is released with the Llama2 license and if in your submission the generated dataset is only consumed by Llama2. Other models have similar licenses like qwen
* You can generate a dataset using internlm (apache 2 license) to finetune any other LLM on the approved model list


<br>

## Evaluation:

<p style='text-align: justify;'>

The evaluation process in our competition will be conducted in two stages. In the first stage, we will run a subset of HELM benchmark along with a set of secret holdout tasks. The holdout tasks will consist of logic reasoning type of multiple-choice Q&A scenarios as well as conversational chat tasks. Submissions will be ranked based on their performance across all tasks. The ranking will be determined by the geometric mean across all evaluation tasks. This score will be shown in the leaderboard. For the most up-to-date details on which specific HELM tasks we're evaluating, please parse the `.conf` files in our starter repo https://github.com/llm-efficiency-challenge/neurips_llm_efficiency_challenge. Keep in mind that your submission needs to take at most 2 hours on the sample `.conf` files we've provided. There are also some hardware constraints that we'll have in place for practical reasons simply because that's the hardware that the organizers have available 128GB of RAM and 500GB of Disk

<br><br>

$$\text{score} = \Pi ( \text{mean-win-rate(\text{task})} )$$

<br><br>

After the competition is closed on October 25th 2023, we will contact the top 3 teams with the highest scoring models in each hardware category, requesting that they submit all necessary code and data to reproduce their model, starting from their chosen open-source base model. We will then replicate their entire process, to ensure it is repeatable and same results can be achieved with 24 hours using a single GPU. If the top-scoring model cannot be reproduced under these imposed conditions, we will move on to consider the next highest-scoring model in the hardware category, we will continue this process until a reproducible and high-performing model is selected, or we exhaust all potential options and declare no winners for the category.

</p> -->
