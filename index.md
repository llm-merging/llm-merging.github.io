---
layout: default
---
Join our discord <a href="https://discord.gg/dPBHEVnV">here</a> !

## Aims and Focus
<p style='text-align: justify;'>
Training high-performing large language models (LLMs) from scratch is a notoriously expensive and difficult task, costing hundreds of millions of dollars in compute alone. These pretrained LLMs, however, can cheaply and easily be adapted to new tasks via fine-tuning, leading to a proliferation of models that suit specific use cases. Recent work has shown that specialized fine-tuned models can be rapidly **merged** to combine capabilities and generalize to new skills. 
</p>

## Get started

The competition will provide the participants with a list of **expert** models that have already been trained on a task-specific dataset. All of these models will be publicly available on the Hugging Face Model Hub with licenses that permit their use for research purposes. These models can either be fully fine-tuned models or models obtained by parameter-efficient fine-tuning methods such as LoRA. Models on this list will be required to satisfy the following criteria: (1) model size $\leq$ 8B parameters, and (2) model with licenses compatible with research use (e.b., [MIT](https://spdx.org/licenses/MIT.html), [Apache 2](https://www.apache.org/licenses/LICENSE-2.0) etc).


<p style='text-align: justify;'>
A starter kit with an end-to-end submission flow can be found here:<br>

[https://github.com/llm-merging/LLM-Merging](https://github.com/llm-merging/LLM-Merging)
</p>

<p>For more details, please check the <a href="challenge.html">Challenge</a>, <a href="rules.html">Rules</a>, and <a href="starter_kit.html">Starter Kit</a> tab.</p>

## Important Dates

<table class="foo">
    <tr>
        <td width="50%"><b>Submission Open</b></td>
        <td width="50%">Early June, 2024</td>
    </tr>
    <tr>
        <td width="50%"><b>Submission Deadline</b></td>
        <td width="50%">Mid-September, 2024</td>
    </tr>
    <tr>
        <td width="50%"><b>Winners Notification</b></td>
        <td width="50%">Beginning of November, 2024</td>
    </tr>
    <tr>
        <td width="50%"><b>Competition Presnetation</b></td>
        <td width="50%">December, 2024</td>
    </tr>
</table>


<!-- <br>

This raises the question: given a new suite of desired skills and design parameters, is it necessary to fine-tune or train yet another LLM from scratch, or can similar existing models be re-purposed for a new task with the right selection or merging procedure? 

<br>

<p style='text-align: justify;'>

The LLM Merging challenge aims to spur the development and evaluation of methods for merging and reusing existing models to form stronger new models without needing additional training. Specifically, the competition focuses on merging existing publicly-released expert models from Huggingface, using only minimal compute and additional parameters. The goal will be to develop merged models that outperform existing models and existing merging baselines. Submissions will be judged based on the average accuracy on a set of held-out multiple-choice evaluation tasks. 
To make the competition as accessible as possible and ensure that the merging procedures are more efficient than fine-tuning, we will enforce a compute budget and focus on merging models with fewer than 8B parameters. A starter kit with all necessary materials (baseline implementations, requirements, the evaluation script, etc.) will be released on May 1st. 
</p> -->


<!-- ## Organizing Institutions

<table cellspacing="0" cellpadding="0" style="border-collapse: collapse;">
    <tr>
        <td style="text-align: center; border: none;"><img src="assets/fig/hf-logo-with-title.png" width="300"></td>
        <td style="border: none;"><img src="assets/fig/sakana_logo.png" width="250"></td>
        <td style="border: none;"><img src="assets/fig/arceeai_transparent background.svg" width="300"></td>
    </tr>

</table> -->


## Sponsors

<table cellspacing="0" cellpadding="0" style="border-collapse: collapse;">
    <tr>
        <td style="text-align: center; border: none;"><img src="assets/fig/hf-logo-with-title.png" width="300"></td>
        <td style="border: none;"><img src="assets/fig/sakana_logo.png" width="250"></td>
        <td style="border: none;"><img src="assets/fig/arceeai_transparent background.svg" width="300"></td>
    </tr>
    <!-- <tr>
                <td style="border: none;"><img src="https://github.com/llm-efficiency-challenge/llm-efficiency-challenge.github.io/assets/3282513/7185238e-b21c-4d82-91f3-86d3465523db" width="300"></td>
        <td style="border: none;"><img src="https://github.com/llm-efficiency-challenge/llm-efficiency-challenge.github.io/assets/3282513/c227bd00-a396-49a5-928c-1d40482508a8" width="300"></td>
        <td style="border: none;"></td>
    </tr> -->
</table>
